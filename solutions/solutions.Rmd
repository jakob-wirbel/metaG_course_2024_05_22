---
title: "AI Applications in Infection Biology - Solutions"
author: "Jakob Wirbel and Georg Zeller"
date: "2024-05-21"
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
    keep_md: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup

First, we have to load the same packages and data.

```{R prep, message=FALSE}
library("tidyverse") # for general data wrangling and plotting
library("SIAMCAT")   # for statistical and ML analyses

data.loc <- 'https://zenodo.org/api/files/d81e429c-870f-44e0-a44a-2a4aa541b6c1/'
fn.feat.fr  <- paste0(data.loc, 'specI_Zeller.tsv')
feat.fr  <- read.table(fn.feat.fr, sep='\t', quote="",
    check.names = FALSE, stringsAsFactors = FALSE)
feat.fr <- as.matrix(feat.fr)
feat.fr.rel <- prop.table(feat.fr, 2)
fn.meta.fr  <- paste0(data.loc, 'meta_Zeller.tsv')
df.meta <- read.table(fn.meta.fr)
```

We will also re-train the model quickly to have it available for the later
exercises:
```{r train, message=FALSE}
sc.obj <- siamcat(feat=feat.fr.rel, meta=df.meta, 
                  label='Group', case='CRC')
sc.obj <- filter.features(sc.obj, filter.method = 'prevalence', cutoff = 0.05)
sc.obj <- normalize.features(sc.obj, norm.method = 'log.std',
                             norm.param = list(log.n0=1e-06, sd.min.q=0))
sc.obj <- create.data.split(sc.obj, num.folds = 10, num.resample = 10)
sc.obj <- train.model(sc.obj, method='lasso')
sc.obj <- make.predictions(sc.obj)
sc.obj <- evaluate.predictions(sc.obj)
```

# Exercise block 1: Data visualization

## Exercise 1-1

> What could be a good effect size for microbiome data? Calculate the fold
change between groups and plot a volcano plot. What do you observe?

First, let's re-calculate the p-values as before

```{r exercise1_1_prep}
feat.fr.rel <- prop.table(feat.fr, 2)
f.idx <- names(which(rowMeans(feat.fr.rel != 0) > 0.05))
f.idx <- setdiff(f.idx, 'UNMAPPED')
feat.fr.rel.filt <- feat.fr.rel[f.idx,]
p.vals <- rep_len(1, nrow(feat.fr.rel.filt))
names(p.vals) <- rownames(feat.fr.rel.filt)
stopifnot(all(rownames(df.meta) == colnames(feat.fr.rel.filt)))
for (i in rownames(feat.fr.rel.filt)){
  x <- feat.fr.rel.filt[i,]
  y <- df.meta$Group
  t <- wilcox.test(x~y)
  p.vals[i] <- t$p.value
}
```

Now, we can compute the fold change between groups in a similar way:
```{r exercise1_1_fc}
fc_values <- rep_len(0, nrow(feat.fr.rel.filt))
names(fc_values) <- rownames(feat.fr.rel.filt)
stopifnot(all(rownames(df.meta) == colnames(feat.fr.rel.filt)))
for (i in rownames(feat.fr.rel.filt)){
  x <- feat.fr.rel.filt[i,]
  med.ctr <- median(x[rownames(df.meta[df.meta$Group=='CTR',])])
  med.crc <- median(x[rownames(df.meta[df.meta$Group=='CRC',])])
  fc_values[i] <- med.ctr - med.crc
}
```

Now we can plot a volcano plot:
```{r exercise1_1_volcano}
plot(fc_values, -log10(p.vals), xlab='Fold change',
     ylab='-log10(P-value)', main='Volcano plot')
```

Interestingly, the median fold change seems a poor effect size estimator in
microbiome data. Many of the bacteria with significant changes between the 
groups have a fold change of 0, because they are relatively rare in either 
group (see also the `Fusobacterium` relative abundance plot from the lesson). 
In the next exercise, we will explore the generalized fold change
that is used in `SIAMCAT`.

## Exercise 1-2

> You can perform association testing with the `SIAMCAT` package as well. The
results are stored in the `SIAMCAT` object and can be extracted by using
`associations(sc.obj)`, if you want to have a closer look at the results for
yourself. Plot a volcano plot of the associations between cancer and controls 
using the output from `SIAMCAT`.

```{r exercise1_2}
sc.obj <- check.associations(sc.obj)
head(associations(sc.obj))
volcano.plot(sc.obj, fn.plot='../figures/volcano.pdf')
```
![](../figures/volcano.png)

The generalized fold change has a better resolution for differences between
the groups, with the `Fusobacterium` species all displaying large fold changes.

## Exercise 1-3

> Create a ordination plot for our data and colour the samples 
by group. How would you interpret the results? Try out different ecological 
distances. How does the choice of distance affect the group separation?

```{r ordination, message=FALSE}
library("vegan")  # for distance calculations
library("labdsv") # easy pco functions

bc.dist <- vegdist(t(feat.fr[rownames(feat.fr.rel.filt),]), method='bray')
pco.res <- pco(bc.dist)
df.pco <- as.data.frame(pco.res$points)
df.pco$Group <- df.meta[rownames(df.pco), 'Group']
df.pco %>% 
  ggplot(aes(x=V1, y=V2, col=Group)) + 
    geom_point() + 
    xlab('PCo 1') + ylab("PCo 2") + 
    theme_bw()

```

In the global "bird's-eye" view of the data, there does not seem to be a big
difference between the two groups. Note that the visible differences in the 
PCOA are usually driven by the most abundant features (such as _B dorei_) and 
these seem to not be very different between CRC and CTR.

Alternatively, we can also have a look at the log-Euclidean distance (instead
of the more traditional Bray-Curtis distance), which gives a very similar 
result:

```{r ordination2}
logE.dist <- vegdist(t(log10(feat.fr.rel.filt + 1e-06)), 
                     method='euclidean')
pco.res <- pco(logE.dist)
df.pco <- as.data.frame(pco.res$points)
df.pco$Group <- df.meta[rownames(df.pco), 'Group']
df.pco %>% 
  ggplot(aes(x=V1, y=V2, col=Group)) + 
    geom_point() + 
    xlab('PCo 1') + ylab("PCo 2") + 
    theme_bw()

```

# Exercise block 2: Machine learning variations

## Exercise 2-1
>We used the `log.std` normalization for our example. Try out different 
normalization procedures and observe the effect on model performance

## Exercise 2-2
>How does the model performance change if you use another machine learning
algorithm? Do the different algorithms select different features?

We can answer both of these questions together by exploring the impact of 
various normalization and machine learning algorithm choices together. We
will change the cross-validation a bit, since training all of these models
will take a long time otherwise:

**Warning** this takes quite a while to run! Especially the `randomForest` is
rather slow

```{r ml_parameter_space, eval=FALSE}
sc.obj.ml <- sc.obj
sc.obj.ml <- create.data.split(sc.obj.ml, num.folds = 5, num.resample = 5)
result.list <- list()
for (norm in c('log.std', 'std', 'pass', 'log.unit')){
  for (ml.method in c('lasso', 'ridge', 'randomForest')){
    message(norm, '-', ml.method)
    sc.obj.active <- sc.obj.ml
    sc.obj.active <- normalize.features(
      sc.obj.active, norm.method = norm,
      norm.param = list(log.n0=1e-06, sd.min.q=0, n.p=2, norm.margin=1),
      verbose = 0)
    sc.obj.active <- train.model(sc.obj.active, method=ml.method, verbose = 1)
    sc.obj.active <- make.predictions(sc.obj.active, verbose = 0)
    sc.obj.active <- evaluate.predictions(sc.obj.active, verbose = 0)
    result.list[[paste0(norm, '_', ml.method)]] <- sc.obj.active
  }
}
```

Now we can compare the AUROC values for each combination of ML algorithm and
normalization procedure:
```{r ml_compare, eval=FALSE}
df.plot <- map(names(result.list), .f=function(x){
  tibble(type=x, auroc=as.numeric(eval_data(result.list[[x]])$auroc))}) %>%
  bind_rows() %>% 
  separate(type, into=c('norm', 'ml.method'), sep='_')
df.plot %>% 
  mutate(l=sprintf('%.2f', auroc)) %>% 
  ggplot(aes(x=norm, y=ml.method, fill=auroc)) + 
    geom_tile() + theme_minimal() + 
    xlab('Normalization method') + ylab('ML algorithm') + 
    scale_fill_gradientn(colours=viridis::viridis(7), limits=c(0.5, 1)) + 
    geom_text(aes(label=l))
```

![](../figures/ml_comparison.png)

You can explore the selected features by going through the list and looking at 
the model interpretation plots.

# Exercise block 3: Predictions on external data

The same Zenodo repository also contains data from another CRC microbiome study
by [Yu et al.](https://gut.bmj.com/content/66/1/70.short). The participants for
this study were recruited in Austria, so you can read in the data by using 
these paths to the data:
```{r external}
fn.meta.at  <- paste0(data.loc, 'meta_Yu.tsv')
fn.feat.at  <- paste0(data.loc, 'specI_Yu.tsv')
```

## Exercise 3-1
>Apply the trained model on this dataset and check the model performance 
on the external dataset.

For this, we first build a new SIAMCAT object for the external data and can 
then apply the original model to it. Note that we don't need to filter it
because SIAMCAT will use only those features present in the original model.

```{r ext_predictions}
feat.at  <- read.table(fn.feat.at, sep='\t', quote="",
    check.names = FALSE, stringsAsFactors = FALSE)
feat.at <- as.matrix(feat.at)
feat.at.rel <- prop.table(feat.at, 2)
df.meta.at <- read.table(fn.meta.at)
sc.obj.at <- siamcat(feat = feat.at.rel, meta=df.meta.at, label='Group',
                     case='CRC')
sc.obj.at.ext <- make.predictions(sc.obj, siamcat.holdout = sc.obj.at)
sc.obj.at.ext <- evaluate.predictions(sc.obj.at.ext)
sc.obj.at.ext
```
The model performs pretty well on the new data with an AUROC of ~0.85

## Exercise 3-2
>Train a `SIAMCAT` model on the Austrian dataset and apply it to the French 
dataset. How does the model transfer on the external dataset compare between
the two datasets? Compare also the feature weights when training on the French
or Austrian dataset.  

```{r at_training}
sc.obj.at <- filter.features(sc.obj.at, 
                             filter.method = 'prevalence', cutoff = 0.05)
sc.obj.at <- normalize.features(sc.obj.at, norm.method = 'log.std',
                             norm.param = list(log.n0=1e-06, sd.min.q=0))
sc.obj.at <- create.data.split(sc.obj.at, num.folds = 10, num.resample = 10)
sc.obj.at <- train.model(sc.obj.at, method='lasso')
sc.obj.at <- make.predictions(sc.obj.at)
sc.obj.at <- evaluate.predictions(sc.obj.at)

model.evaluation.plot('FR'=sc.obj, 'AT'=sc.obj.at, 
                      fn.plot = '../figures/eval_at_fr.pdf')
```

![](../figures/eval_at_fr.png)
We can also apply this new model to the original French data:
```{r cross_application}
sc.obj.fr.ext <- make.predictions(sc.obj.at, sc.obj)
sc.obj.fr.ext <- evaluate.predictions(sc.obj.fr.ext)

model.evaluation.plot('FR'=sc.obj, 'AT'=sc.obj.at, 
                      'AT-->FR'=sc.obj.fr.ext,
                      'FR-->AT'=sc.obj.at.ext,
                      fn.plot = '../figures/eval_cross_application.pdf')
```

![](../figures/eval_cross_application.png)
The models, in general, transfer well across datasets. However, the model 
trained on the AT data does show a decrease in performance on the FR data and
generalizes less well. 
The feature weights are --surprisingly-- pretty different between the two 
datasets:

```{r feat_weights}
weights.fr <- feature_weights(sc.obj)
weights.at <- feature_weights(sc.obj.at)

# combine both feature weight information and plot as scatter
df.plot <- weights.fr %>% 
  as_tibble(rownames='feature') %>% 
  select(feature, mean.rel.weight) %>% 
  rename(weights_fr=mean.rel.weight) %>% 
  full_join(weights.at %>% 
    as_tibble(rownames='feature') %>% 
    select(feature, mean.rel.weight) %>% 
    rename(weights_at=mean.rel.weight), by='feature') %>% 
  # fill in NAs
  mutate(weights_fr=replace_na(weights_fr, 0)) %>% 
  mutate(weights_at=replace_na(weights_at, 0))
df.plot %>% 
  ggplot(aes(x=weights_fr, y=weights_at)) + 
    geom_point() + 
    theme_bw() + 
    xlab("Feature weights from the model trained on FR data") + 
    ylab("Feature weights from the model trained on AT data")

df.plot %>% 
  arrange(weights_fr) %>% head
df.plot %>% 
  arrange(weights_at) %>% head
```

# SessionInfo

```{r session_info}
sessionInfo()
```
